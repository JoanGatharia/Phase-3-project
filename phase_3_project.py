# -*- coding: utf-8 -*-
"""Phase 3 Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cPVSOs2aKEF9cMid3GBajeol8sA92U-y

# Business Understanding

- Customer churn(Shows if a customer left/stopped using the service) is a critical issue in the telecom industry, directly impacting revenue, customer acquisition costs, and profitability. Identifying at-risk customers and implementing retention strategies is key to sustaining business growth.

## Problem  Statement
- The company is losing customers, and it needs a way to predict churn before it happens. Understanding why customers leave will help in making data-driven decisions to improve retention.Thus in summary:

 - Regression task: Predict numerical factors that influence churn, such as service usage, customer complaints, or charges
 - Classification task(Main): Predict whether a customer will churn

## Objectives

***General***
1. Predict churn with a classification model to identify high-risk customers
2. Be able to identify the main churn drivers such as service plans and customer service interactions
3. Come up with acttionable retention strategies based on the model's insights
4. Improve customer service by analyzing the impact of service interactions on churn
5. Enhance loyalty and marketing strategies through targeting high churn risk customers with personalized offers

***Regression***

- Identify heavy users who might be at risk of churn if they are dissatisfied with pricing or service.
- Identify customers likely to make multiple complaints, which could signal dissatisfaction before they churn.
-  identifying whether high-bill customers are more likely to churn.
- Predicting churn probability to allow the company to rank customers by churn risk

***Classification***

 -  identify at-risk customers and implement retention strategies.
 - Perform EDA to understand class distribution.
 - Build and train a classification model to predict customer churn
 - Optimize model performance through feature engineering and hyperparameter tuning then select the best performing classifier using F1-score

## Research Questions
1. What are the most significant factors influencing customer churn?

2. Does frequent customer service interaction indicate a higher risk of churn?

3. Do customers with an international plan have a higher churn rate?

4. Can a machine learning model accurately predict churn using available features?

# Data Understanding
"""

#Import required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
from scipy.stats import chi2_contingency
from scipy.stats import zscore
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report, confusion_matrix
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV

#Load the dataset
df = pd.read_csv('Churn_tel_data.csv')
df.head() #Evaluate the first 5 rows of the data set to get the general overview

#Check the statistical summary of the data set
df.describe()

df.info() #To get the general information on the data such as the column names the number of columns and the data types

df.shape #To get the number of columns and rows in the entire dataset

#We can check for the unique values especially in categorical data columns
for col in df.select_dtypes(include='object'):
     print(f"{col}: {df[col].nunique()} unique values")
     print(df[col].unique()[:10])  # Display the first  unique values
     print("-" * 40)

"""# Data Cleaning & Preprocessing

## Correct Formats
"""

#For correct format first Observe the data type
print(df.dtypes)

#Convert categorical variables to numeric
#Convert 'yes'/'no' categorical variables to binary (0/1)--- Label encoding

df['international plan'] = df['international plan'].map({'yes': 1, 'no': 0})
df['voice mail plan'] = df['voice mail plan'].map({'yes': 1, 'no': 0})

#Convert churn from boolean to integer (0/1)
df['churn'] = df['churn'].astype(int)

"""- Reason as to why we convert categorical variables to numeric such as churn from boolean to integer is to:
  - Ensure that we have consistency to avoid unexpected error when training machine learning models
  - To make analysis easier especially when computing metrics
  - Improve model performance
"""

#Drop the column Phone number since it is not useful for prediction
df.drop(columns=['phone number'], inplace=True)

#Run data types again to confirm the changes
print(df.dtypes)

#To determine if to drop the column state and area code, we need to see if churn rates vary significantly across the states and area codes
#Churn rate per state
state_churn_rate = df.groupby('state')['churn'].mean().sort_values(ascending=False)
print(state_churn_rate)

#Churn rate per area code
area_code_churn_rate = df.groupby('area code')['churn'].mean().sort_values(ascending=False)
print(area_code_churn_rate)

"""- we see churn rates are almost similar across all area codes this makes it unuseful for prediction thus we will drop it
- For churn across different states it varies which makes the column state somewhat useful thus we will encode it
"""

#we can do abit of Chi-Square Test to confirm the significant relationship between churn and (state,area code)

#First create contingency tables
state_contingency = pd.crosstab(df['state'], df['churn'])
area_code_contingency = pd.crosstab(df['area code'], df['churn'])

#Run chi_square test
state_chi2, state_p, state_dof, state_expected = chi2_contingency(state_contingency)
area_code_chi2, area_code_p, area_code_dof, area_code_expected = chi2_contingency(area_code_contingency)

print(f"State Chi-Square Statistic: {state_chi2}, p-value: {state_p}")
print(f"Area Code Chi-Square Statistic: {area_code_chi2}, p-value: {area_code_p}")

"""- Interpreting the output:
  - state vs churn chi-square statistic is 83.4 and p-value of 0.0023 thus < 0.05 showing a significant impact on churn meaning we retain the column
  - Area code vs churn chi-square statistic is 0.18, p-value of 0.9151 thus > than 0.05 showing no significant impact meaning we drop the column

"""

#encode state column --- one hot encoding
df = pd.get_dummies(df, columns=['state'], drop_first=True)

#drop area code column
df.drop(columns=['area code'], inplace=True)

"""## Handling NAs (Missing values)"""

df.isnull().sum()

"""- We have no missing values in our data

## Handling Duplicates
"""

df[df.duplicated()].count()

"""  - We do not have duplicated values either"""

##Check data Distribution plot histograms
df.hist(figsize=(12,10), bins=30, edgecolor="black")
plt.suptitle("Feature Distributions", fontsize=16)
plt.show()

#Pie chart for categorical distribution
df['churn'].value_counts().plot.pie(autopct='%1.1f%%', colors=['lightblue', 'salmon'])
plt.title("Churn Distribution")
plt.ylabel("")  # Hide y-label
plt.show()

#violin plot for churn VS customer service calls
plt.figure(figsize=(8, 5))
sns.violinplot(x="churn", y="customer service calls", data=df, palette="muted")
plt.title("Churn vs. Customer Service Calls")
plt.show()

"""## Outliers
- For this we need to use the visual approach such as boxplots just to see outliers

### Box Plots
"""

#Check outliers by plotting a box plot for all numerical columns
plt.figure(figsize=(10, 6))
sns.boxplot(data=df.drop(columns=['state']))
plt.title('Box Plot of Numerical Columns')
plt.xticks(rotation=90)# rotate for better visibility
plt.show()

"""- Multiple outliers in several features are detected.
  - Total day minutes, total eve minutes, total night minutes show extreme outliers
  - Total intl calls has fewer outliers but still needs to be worked on.
  - Area code is categorical so should not be analyzed
"""

#Use Z-score to count outliers on numerical columns
z_scores = np.abs(zscore(df.select_dtypes(include=[np.number])))
threshold = 3
outlier_count = (z_scores > threshold).sum()
print(outlier_count)

"""- The columns with extreme outliers > 10 need careful handling
- We can either;
   - Remove the outliers using z-score filtering
   - Apply log transformation- but check if data is skewed first
"""

# Remove rows where any numeric column has a z-score above 3 or below -3
numeric_columns = df.select_dtypes(include=[np.number]).columns
df = df[(np.abs(zscore(df[numeric_columns])) < 3).all(axis=1)]

#check skewness
# Include only numeric columns for skewness calculation
numeric_df = df.select_dtypes(include=np.number)
df_skew = numeric_df.skew()
print(df_skew)

#Apply log transformation to skewed columns
skewed_columns = ['total day minutes', 'total eve minutes', 'total night minutes']
for col in skewed_columns:
    df[col] = np.log1p(df[col])

#We check the distribution again
#Select the transformed columns
transformed_columns = ['total day minutes', 'total eve minutes', 'total night minutes']

#Now we plot the histogram
plt.figure(figsize=(12, 6))
for i, col in enumerate(transformed_columns, 1):
    plt.subplot(1, len(transformed_columns), i)
    sns.histplot(df[col], kde=True)
    plt.title(f'Histogram of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

"""- We still can spot abit of outliers we can perform zscore again to see if the count reduced"""

#zscore after log transformation
z_scores = np.abs(zscore(df.select_dtypes(include=[np.number])))
threshold = 3
outlier_count = (z_scores > threshold).sum()
print(outlier_count)

"""- The outlier count reduced meaning log transformation helped

"""

#Box plot check after log transformaion
plt.figure(figsize=(10, 6))
sns.boxplot(data=df.drop(columns=['state']))
plt.title('Box Plot of Numerical Columns')
plt.xticks(rotation=90)# rotate for better visibility
plt.show()

"""# Exploratory Data Analysis

## Univariate Analysis

 - Histograms and Boxplots
"""

# Histograms for numerical features
plt.figure(figsize=(15, 10))
for i, col in enumerate(df.select_dtypes(include=np.number).columns):
    plt.subplot(4, 4, i + 1)
    sns.histplot(df[col], kde=True)
    plt.title(col)
plt.tight_layout()
plt.show()

# Boxplots for numerical features
plt.figure(figsize=(15, 10))
for i, col in enumerate(df.select_dtypes(include=np.number).columns):
    plt.subplot(4, 4, i + 1)
    sns.boxplot(y=df[col])  # Use y= for vertical boxplots
    plt.title(col)
plt.tight_layout()
plt.show()

# Bar plots for categorical features
for col in df.select_dtypes(include='object').columns:
    plt.figure(figsize=(8, 6))
    df[col].value_counts().plot(kind='bar')
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.show()

"""## Bivariate Analysis"""

# bivariate analysis visualization churn vs total day minutes
# Set up the figure and axes
plt.subplot(2, 2, 1)
sns.scatterplot(x=df["total day minutes"], y=df["total day charge"], hue=df["churn"], palette="coolwarm", alpha=0.6)
plt.title("Total Day Minutes vs. Total Day Charge (Colored by Churn)")
plt.xlabel("Total Day Minutes")
plt.ylabel("Total Day Charge")
plt.show()

#Churn vs Customer Service Calls
plt.subplot(2, 2, 2)
sns.boxplot(x="churn", y="customer service calls", data=df, palette="coolwarm")
plt.title("Customer Service Calls by Churn Status")
plt.xlabel("Churn")
plt.ylabel("Customer Service Calls")
plt.show()

#  Churn vs Total Intl Minutes & Total Intl Calls
plt.subplot(2, 2, 3)
sns.scatterplot(x=df["total intl minutes"], y=df["total intl calls"], hue=df["churn"], palette="coolwarm", alpha=0.6)
plt.title("Total Intl Minutes vs. Total Intl Calls (Colored by Churn)")
plt.xlabel("Total Intl Minutes")
plt.ylabel("Total Intl Calls")
plt.show()

#  Churn vs. Total Evening Usage
plt.subplot(2, 2, 4)
sns.boxplot(x="churn", y="total eve minutes", data=df, palette="coolwarm")
plt.title("Total Evening Minutes by Churn Status")
plt.xlabel("Churn")
plt.ylabel("Total Evening Minutes")
plt.show()

"""## Multivariate + Analysis"""

# Pairplot for multivariate analysis
sns.pairplot(df, hue='churn', vars=['total day minutes', 'total eve minutes', 'total night minutes', 'total intl calls', 'customer service calls'], palette='Set1')
plt.suptitle('Pairplot of Key Features Colored by Churn', y=1.02)
plt.show()

"""# Feature Engineering
 - This helps improve model accuracy

### Encoding
"""

#Here we get to use either one hot encoding or label encoding since we are converting categorical data to numerical data as seen below
#label encoding
df['international plan']= df['international plan'].map({'yes': 1, 'no': 0})
df['voice mail plan'] = df['voice mail plan'].map({'yes': 1, 'no': 0})

#One hot encoding esp for multiple category variables
df = pd.get_dummies(df, columns=['state'], drop_first=True)

#try creating new features to improve predictive power
#Derived code from the internet
df['day_call_ratio'] = df['total day calls'] / df['total day minutes']
df['eve_call_ratio'] = df['total eve calls'] / df['total eve minutes']
df['night_call_ratio'] = df['total night calls'] / df['total night minutes']

df['total minutes'] = df['total day minutes'] + df['total eve minutes'] + df['total night minutes'] + df['total intl minutes']

df['high_service_calls'] = (df['customer service calls'] > 3).astype(int)

"""- We are trying to identify customers with high risk of churn indicated by customers with > than 3 service calls.

#### Feature Selection
- Helps avoid multicollinearity thus improving model efficiency
"""

# Identify the column with the problematic value '382-4657'
problematic_column = df.apply(lambda x: x.astype(str).str.contains('382-4657').any()).idxmax()
print(f"Problematic column: {problematic_column}")

# drop the problematic column
df = df.drop(columns=[problematic_column])

# Compute correlation matrix, excluding non-numeric columns
corr_matrix = df.select_dtypes(include=np.number).corr().abs()

# Select upper triangle of correlation matrix
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))

# Find features with correlation greater than 0.9
to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]

print("Dropping these features due to high correlation:", to_drop)

# Drop highly correlated features- redundant features
df = df.drop(columns=to_drop)

#Correlation Heatmap
numerical_features = df.select_dtypes(include=['number'])
plt.figure(figsize=(12, 10))
sns.heatmap(numerical_features.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap (Numerical Features)')
plt.show()

"""- there appears to be a class imbalance in the 'churn' variable, with significantly more non-churned customers than churned customers.  
- This imbalance needs to be addressed during model training.  Several numerical features show some correlation with 'churn', suggesting they could be useful predictors.  
- The categorical features 'international plan' and 'voice mail plan' also appear to have some influence on churn.
- Missing values are not present in the dataset. Given these observations, a machine learning model may be able to predict churn with reasonable accuracy, but careful model selection, feature engineering, and handling of the class imbalance are crucial for optimal performance.
"""

# Compute correlation matrix
corr = df.corr()

# Mask upper triangle for better visualization
mask = np.triu(np.ones_like(corr, dtype=bool))

# Set figure size
plt.figure(figsize=(12, 8))

# Draw the heatmap
sns.heatmap(corr, mask=mask, cmap="coolwarm", annot=False, fmt=".2f", linewidths=0.5)

plt.title("Filtered Correlation Matrix")
plt.show()

"""### Feature Scaling
- This is basically for normalization for models like K-NN model
- MinMaxScaler scales values between 0-1
"""

scaler = MinMaxScaler()
df[['total day minutes', 'total eve minutes', 'total night minutes', 'total intl minutes']] = scaler.fit_transform(df[['total day minutes', 'total eve minutes', 'total night minutes', 'total intl minutes']])
df.head()

""" - We can also perform standardization using the standard scaler for algorithms like Logistic Regression"""

scaler = StandardScaler()
df[['total day minutes', 'total eve minutes', 'total night minutes', 'total intl minutes']] = scaler.fit_transform(df[['total day minutes', 'total eve minutes', 'total night minutes', 'total intl minutes']])
df.head()

"""# Modeling

## Regression
"""

# Reset index to avoid issues after removing rows for outliers
df = df.reset_index(drop=True)

# Define X and y
X = df.drop(columns=['churn'])
y = df['churn']

# Identify object columns
object_columns = X.select_dtypes(include=['object']).columns

# One-hot encode object columns
if len(object_columns) > 0:
    X = pd.get_dummies(X, columns=object_columns, drop_first=True)  # Use drop_first=True to avoid multicollinearity

# Add a constant to the independent variables
X = sm.add_constant(X)

# Convert boolean columns to integers
X = X.astype({col: 'int' for col in X.select_dtypes(include=['bool']).columns})

# Drop columns containing 'phone number' if they exist
X = X.drop(columns=[col for col in X.columns if 'phone number' in col], errors='ignore')

# One-hot encoding for remaining categorical columns
# Check and convert columns with 'object' dtype to numeric before applying get_dummies
for col in X.select_dtypes(include=['object']).columns:
    X[col] = pd.to_numeric(X[col], errors='coerce') # Convert to numeric, handling errors

X = pd.get_dummies(X, drop_first=True, dummy_na=False)  # dummy_na=False to avoid creating dummies for NaN

# Convert to numeric, replacing inf and NaN with a large and small number
X = X.apply(pd.to_numeric, errors='coerce').replace([np.inf, -np.inf], np.nan)
X.fillna(X.mean(), inplace=True) # Impute NaN with column means

print(X.isnull().sum().sum())  # Total NaN count
print(X.isnull().sum())  # Check per column

X = X.apply(pd.to_numeric, errors='coerce')  # Convert all non-numeric to NaN
X.fillna(X.mean(), inplace=True)  # Fill any new NaNs that appeared

print("Remaining NaNs:", X.isnull().sum().sum())
print("Remaining Infs:", np.isinf(X).sum().sum())
print("Data Types:", X.dtypes.unique())  # Should only show int64 and float64

print(X.isnull().sum())  # Check NaNs per column

#Fit the linear regression model
model = sm.OLS(y, X).fit()
print(model.summary())

#Calculate the metrics
y_pred = model.predict(X)
mse = mean_squared_error(y, y_pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y, y_pred)
r2 = r2_score(y, y_pred)

print(f"MSE: {mse}")
print(f"RMSE: {rmse}")
print(f"MAE: {mae}")
print(f"R²: {r2}")

"""- The above RMSE output means that the model is making prediction errors of around 31% on average
- An R² of 0.2221 that is 22.21% can be explained by the models features however they may not be strong predictors of churn
- MAE suggests that the model on average is 20% off from the true values of churn
  
  - The above shows that churn is a binary classification problem not continuous hence the model is not capturing enough variance

## Classification

### Logistic Regression
"""

#Define features X and target y
X = df.drop(columns=['churn'])
y = df['churn']

#Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Impute missing values using SimpleImputer
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')  # Replace NaNs with the mean of the column

# Fit the imputer on the training data and transform both training and testing data
X_train = imputer.fit_transform(X_train)
X_test = imputer.transform(X_test)

#Initialize and train the logistic regression model
log_reg_model = LogisticRegression()
log_reg_model.fit(X_train, y_train)

#Make predictions on the test set
y_pred = log_reg_model.predict(X_test)

#Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print(f"Recall: {recall}")
print(f"Precision: {precision}")
print(f"F1 Score: {f1}")

#Classification report
print(classification_report(y_test, y_pred))

#Plot a confusion matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Churn', 'Churn'], yticklabels=['No Churn', 'Churn'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

#Decode the predictions and true labels
y_pred_decoded = np.where(y_pred == 1, 'Churn', 'No Churn')
y_test_decoded = np.where(y_test == 1, 'Churn', 'No Churn')

#plot the confusion matrix with decoded labels
cm= confusion_matrix(y_test_decoded, y_pred_decoded, labels=['No Churn', 'Churn'])
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Churn', 'Churn'], yticklabels=['No Churn', 'Churn'])
plt.title('Confusion Matrix (Decoded)')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""- From the Logistic regression model results we get a good accuracy of 84.7%
- Recall however is very low at 5% meaning the model is missing a lot of customers who actually churn
- F1-Score is low as well indicating a poor balance
- Prediction is at 55% which is moderate
- Let's try another model
"""

#Tune the logistic model
#Define the parameter grid to search
param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],'penalty': ['l1', 'l2'], 'solver': ['liblinear', 'saga']
}

#Initialize GridSearchCV
grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='accuracy')

#Fit the grid search to the data
grid_search.fit(X_train, y_train)

#Get the best hyperparameters and best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

print(f"Best Hyperparameters: {best_params}")
print(f"Best Accuracy: {best_score}")

#Train a new model with the best hyperparameters
best_model = LogisticRegression(**best_params)
best_model.fit(X_train, y_train)

#Make predictions on the test set
y_pred = best_model.predict(X_test)

#Evaluate the tuned model
accuracy = accuracy_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

"""- Accuracy moved up to 86.55% which was an improvement
- Penalty L2 that is Ridge regression helps prevent overfitting
- C= 0.01 controls regularization and smaller values give stronger regularization
"""

#classification report
print(classification_report(y_test, y_pred))

"""- Model seems not to be classifying churn at all thus not suitable for churn prediction

### Decision Tree
"""

#Initialize and train the decision tree classsifier
dt_model = DecisionTreeClassifier()
dt_model.fit(X_train, y_train)

#Make predictions on the test set
y_pred = dt_model.predict(X_test)

#Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print(f"Recall: {recall}")
print(f"Precision: {precision}")
print(f"F1 Score: {f1}")

#Classification report
print(classification_report(y_test, y_pred))

#Plot a confusion matrix
cm= confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Churn', 'Churn'], yticklabels=['No Churn', 'Churn'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

#Plot decision tree
from sklearn.tree import plot_tree

#Assuming dt_model is your trained DecisionTreeClassifier
plt.figure(figsize=(15, 10))
plot_tree(dt_model, filled=True, feature_names=X.columns, class_names=['No Churn', 'Churn'], rounded=True, proportion=True)
plt.show()

"""- Accuracy is at 90.85% which means the model is performing well
- Recall is at 70% which means the model is getting 70% of actual churn cases
- Precision is at 70% as well which means when the model predicts churn it is correct 70% of the time
- F1-score is at 70% as well showing a well optimized model for predicting churn
"""

#Tune the decision tree model
#Define the parameter grid
param_grid = {
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

#Initialize GridSearchCV
grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5, scoring='accuracy')

#Fit the grid search to the data
grid_search.fit(X_train, y_train)

#Get the best hyperparameters and best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

print(f"Best Hyperparameters: {best_params}")
print(f"Best Accuracy: {best_score}")

#Train a new model with the best hyperparameters
best_model = DecisionTreeClassifier(**best_params)
best_model.fit(X_train, y_train)

#Make predictions on the test set
y_pred = best_model.predict(X_test)

#Evaluate the tuned model
accuracy = accuracy_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

#Print classification report
print(classification_report(y_test, y_pred))

"""- After tuning the model improved its accuracy by 4%
- Precision improved as well up to 79%
- Recall moved down to 69%
- F1-score moved up to 74% showing a balance of precision and recall

### Random Forest
"""

#Initialize and train the random forest classifier
rf_model = RandomForestClassifier()
rf_model.fit(X_train, y_train)

#Make predictions on the test set
y_pred = rf_model.predict(X_test)

#Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print(f"Recall: {recall}")
print(f"Precision: {precision}")
print(f"F1 Score: {f1}")

#Classificattion report
print(classification_report(y_test, y_pred))

#Plot a confusion matrix
cm= confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Churn', 'Churn'], yticklabels=['No Churn', 'Churn'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""- Accuracy has improved to 92.7% with random forest giving more correct predictions overall.
- we have a higher precision 90% meaning when it predicts churn it is highly correct
- A lower recall of 59% however showing the model is missing more churn cases
- F1-score of 71% shows there is balance

- After tuning the Random Forest Model the accuracy is at 93%
- Recall is at 63% which is lower thus the model misses 37% of the churn cases
- F1-score shows random forest is well balanced than the other models
- High precision of 91% may indicate that the model igores false alarms while predicting churn
"""

#Tune the random forest model
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

#Initialize a GridSearchCV
grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, scoring='accuracy')

#Fit the grid search to the data
grid_search.fit(X_train, y_train)

#Get the best hyperparameters and best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

print(f"Best Hyperparameters: {best_params}")
print(f"Best Accuracy: {best_score}")

#Train a new model with the best hyperparameters
best_model = RandomForestClassifier(**best_params)
best_model.fit(X_train, y_train)

#Make predictions on the test set
y_pred = best_model.predict(X_test)

#Evaluate the tuned model
accuracy = accuracy_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

#Classification Report
print(classification_report(y_test, y_pred))

"""### K-NN model"""

#Initialize and train the K-NN classifier
knn_model = KNeighborsClassifier()
knn_model.fit(X_train, y_train)

#Make predictions on the test set
y_pred = knn_model.predict(X_test)

#Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print(f"Recall: {recall}")
print(f"Precision: {precision}")
print(f"F1 Score: {f1}")

#Classification report
print(classification_report(y_test, y_pred))

#Plot confusion matrix
cm= confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Churn', 'Churn'], yticklabels=['No Churn', 'Churn'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""- The accuracy looks good at 82.96% but might be misleading
- Precision is at 14% which means when predicting churn it is only correct 14% of the time
- 2% recall shows the model rarely detects churn cases
- F1-score 0f 3.6% shows very poor balance between precision and recall
"""

#Tune the K-NN model
param_grid = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance'],
    'p': [1, 2]
}

#Initialize GridSearchCV
grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, scoring='accuracy')

#Fit the grid search to the data
grid_search.fit(X_train, y_train)

#Get the best hyperparameters and best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

print(f"Best Hyperparameters: {best_params}")
print(f"Best Accuracy: {best_score}")

#Train a new model with the best hyperparameters
best_model = KNeighborsClassifier(**best_params)
best_model.fit(X_train, y_train)

#Make predictions on the test set
y_pred = best_model.predict(X_test)

#Evaluate the tuned model
accuracy = accuracy_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print(f"Recall: {recall}")
print(f"Precision: {precision}")
print(f"F1 Score: {f1}")

#Classification report
print(classification_report(y_test, y_pred))

"""- Even after tuning the model still does not improve

### SVM
"""

#Define features (X) and (y)
X= df.drop(columns=['churn'])
y= df['churn']

#Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train te SVC model
svm_model = SVC()
svm_model.fit(X_train, y_train)

#Make predictions on the test set
y_pred = svm_model.predict(X_test)

#Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print(f"Recall: {recall}")
print(f"Precision: {precision}")
print(f"F1 Score: {f1}")

#Classification report
print(classification_report(y_test, y_pred))

#Plot the confusion matrix
cm= confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Churn', 'Churn'], yticklabels=['No Churn', 'Churn'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""- Just like the K-NN model, the SVM model also shows poor results when it comes to recall and precision which means the model rarely predicts customer churn

## Hyperparameter Tuning
"""

#So far the best model is the Decision Tree
#Tune the decision tree model
#Define the parameter grid
param_grid = {
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

#Initialize GridSearchCV
grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5, scoring='accuracy')

#Fit the grid search to the data
grid_search.fit(X_train, y_train)

#Get the best hyperparameters and best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

print(f"Best Hyperparameters: {best_params}")
print(f"Best Accuracy: {best_score}")

#Train a new model with the best hyperparameters
best_model = DecisionTreeClassifier(**best_params)
best_model.fit(X_train, y_train)

#Make predictions on the test set
y_pred = best_model.predict(X_test)

#Evaluate the tuned model
accuracy = accuracy_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

#Print classification report
print(classification_report(y_test, y_pred))

"""# Model Evaluation

- The best model to predict customer churn is the tuned Decison Tree Model
- The Accuracy is high at 94.4% meaning good overall prediction
- Precision is  at 79% meaning when it predicts churn it is almost 80% correct
- Recall is still low at 69% meaning the model still misses some churners
- F1-score is well balanced at 74%

#### Class Weighting
"""

# Define custom class weights
# Adjust the ratio based on churn distribution
custom_weights = {0: 1, 1: 3}

# Initialize Decision Tree with custom weights
dt_custom = DecisionTreeClassifier(
    max_depth=10,
    min_samples_split=10,
    min_samples_leaf=2,
    class_weight=custom_weights,
    random_state=42
)

# Train and evaluate
dt_custom.fit(X_train, y_train)
y_pred_custom = dt_custom.predict(X_test)

# Print classification report
print(classification_report(y_test, y_pred_custom))

"""- Accuracy is still strong 90% after class weighting to improve recall
- Recall significantly improved to 72% meaning the model is better at catching churners
- Precision dropped to 68% but that indicates some churners are false positives
- F1-score improved 70% which indicates better balance between recall and precision

# Conclusion and Recommendation

- The Tuned Decision Tree model is better at predicting customer churn given the strong recall and accuracy this model is ready for real world use.
- For the business more customer retention methods/Strategies can be applied early such as personalized offers/discounts or the customer support outreach
- Further model improvemments such as SMOTE to better balance the recall and precision might be needed since a high recall ensures feweer lost customers and thus reducing revenue loss
- As an additional recommendation the company can explore additional features such as customer engagement patterns.

# References
- Canvas Notes
- Fawcett, T. (2006). An introduction to ROC analysis. Pattern Recognition Letters, 27(8), 861-874
- Breiman, L. (2001). Random Forests. Machine Learning, 45(1), 5-32.
- Lemmens, A., & Gupta, S. (2020). Managing Churn to Maximize Profits. Foundations and Trends® in Marketing, 14(1), 1-74.
= OpenAI (2025). AI-assisted insights on churn prediction and machine learning modeling
"""

#Download the clean dataset for tableau
df.to_csv('churn_tel_data.csv', index=False)
from google.colab import files
files.download('churn_tel_data.csv')